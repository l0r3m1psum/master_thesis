% https://en.wikibooks.org/wiki/LaTeX/Document_Structure
\documentclass{report}

\title{The Back-propagation Cookbook}
\author{Diego Bellani}
\date{2024}

\newcommand{\TODO}[1]{TODO: #1}
% https://tex.stackexchange.com/a/284054
\newcommand{\argmin}{\mathop{\mathrm{arg\,min}}}
\newcommand{\trace}{\mathrm{Tr}}

\begin{document}
\maketitle
% \makeindex
\tableofcontents

\begin{abstract}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean
quis justo et arcu elementum venenatis ac vitae dolor. Proin ut pretium diam.
Praesent volutpat commodo est. Integer vitae nulla neque. Sed finibus libero
eget sem imperdiet mollis. Ut fringilla nisi vitae vestibulum facilisis. Nunc
dapibus neque quam, ut convallis turpis maximus ac. Ut lacinia mi elit, quis
ullamcorper dolor vestibulum ac. Maecenas mattis laoreet augue quis ultricies.
Sed tincidunt scelerisque tincidunt.
\end{abstract}

\part{Preliminaries}

\chapter{Machine Learning and Deep Learning}

\section{An Informal Introduction}

Machine learning is a subset of artificial intelligence (AI) in which
statistical models and training algorithms are developed, such that, given a
set of data the training algorithm can specialize the statistical model to
solve a certain task\footnote{If we could specialize a certain model to solve
``any task'' we would call this artificial general intelligence. Quotations are
necessary around ``any task'' due to the fact that algorithm can't solve
literally any problem(e.g. the halting problem).}.

The training algorithm iteratively \emph{teaches} the model at solving the task,
each iteration should improve the ability of the model to perform such task,
this iterative improvement can be thought as a form of \emph{learning}. Hence
the name of this field of study.

This is a fundamental shift of perspective from the Good Old-Fashioned AI
(i.e. the one that based on symbolical logic) were so called ``expert systems''
relied on a hand cured series of facts and inference rules that allowed the
system to reason about a certain problem in the hope of solving it.

The simplest example of a task commonly performed with machine learning is the
one of image classification. In this task you have a set of labels for pictures
(e.g. animal, car, book, etc\dots) and given an input image you have to assign
the correct label to it.

Machine learning in some form or another exits from a lot of time (just think of
inferential statistics). The reason it was only recently became a viable method,
and ultimately the most effective one in many fields, is due to the abundance of
easily available data in digital form (in the order of gibibyte and tebibyte)
and the huge computing power that the development of semi-conductor design and
manufacturing has brought us (again in the order of tera-floating-point
operations per second).

Machine learning has many sub-fields itself but the one that contributed the
most to its success and the end of the AI winter\footnote{This is a term used
in the history of AI to identify periods in which there is a reduction in the
funding and the interest in AI research. This used to happen after AI research
failed to met the inflated expectations that it set multiple times.} (and the
start of the AI spring) is the so called deep learning. This sub-field concerns
itself with a specific king of model: deep artificial neural networks. This
models are composed of layers and are deep in the sense that they have many of
them. Having many layers allows this model to perform their task with minimal
pre-processing on the data and no manual feature extraction. This is opposed to
more classical machine learning models which expect input of data as a vector
of features.

If we think about the image classification example it is already non obvious
which feature to extract from the images to aid a classification algorithm
perform its task. With the deep learning approach we provide the images to the
algorithm, almost, as is and the various layer of the model learn to perform
the various feature extraction sub-tasks that are ultimately used by the
network itself to perform the classification.

The family training algorithms that made many of this progress with deep neural
networks models possible are the one based on mini-batch stochastic gradient
descent (together with the back-propagation algorithm\footnote{The main topic
of this thesis.} to efficiently calculate the gradient). The main idea of this
family of algorithm is to teach the model by correcting it based on the errors
that it makes. This error is quantified with a loss function which the algorithm
tries to minimize, during the course of its execution, with a stochastic
approximation of the gradient of the loss function.

The various kinds of deep learning models are often called \emph{architectures}.
This architectures have characteristic the allow them to better solve certain
kinds of tasks. For example the convolutional neural network are particularly
well suited to solve image related task (like our classification example).

\subsection{Brief history of the recent notable results}

The field of deep learning has accomplished many notable results in the recent
years. We have selected the events that in our opinion give the best idea of
what this modern systems are capable of. Most of this things were unheard of
(or even unthinkable) just a few years ago and the pace at which we are seeing
this progress does not seem to stop (or slow down for that matter).

The progress are not limited to solving or advancing old problems but we now
have totally new kind of computer programs such as programs that can generate
images from prompts, transfer an artist style to a photo, chat programs that
can can perform question answering tasks in a way that previous search engines
never could.

The first big accomplishment for deep learning was 2012 when AlexNet (a
convolutional neural network) won the ImageNet Large Scale Visual Recognition
Challenge (ILSVRC2012) with a significant margin. To give an idea of how hard
this task is the number of classes in this dataset is more than 10,000.

In 2015 Tesla introduced Autopilot a software able to give self-driving
capabilities to commercial Tesla's cars.

In 2016 AlphaGo, which is an program that uses deep learning to guide its Monte
Carlo tree search with a ``value network'' and a ``policy network'', was the
first computer program to defeat the Go world champion. While defeating human at
chess could be done with GOFAI techniques Go is a much harder harder problem to
solve. If we consider the number of possible board states as a proxy for the
% https://math.stackexchange.com/a/1407631
hardness of this games chess has roughly $10^{45}$ possible board states while
% https://en.wikipedia.org/wiki/Go_(game)
Go has $10^{170}$. For reference the number of atoms in the observable universe
is estimated to be in the order of $10^{80}$.

In 2017 the transformer architecture (Attention is All You Need) revolutionized
the natural language processing field. This architecture in 2020 allowed OpenAI
to create GPT-3, the first model powering ChatGPT in 2022.

In 2018 DeepMind developed AlphaFold and in 2020 AlphaFold 2 archived astounding
results in CASP14, basically ``solving'' the protein folding problem.

In 2021 OpenAI released DALL-E a model that allowed to enter a prompt and
generate an image based on that prompt.

In 2022 AlphaTensor improves a matrix multiplication algorithm.

In 2024 (the year of the writing of this thesis) OpenAI announced Sora, a model
able to generate video from a prompt.

There are many more examples that could be given about the impressing
capabilities of modern deep learning systems, that are left out for the sake of
brevity. The ones reported were not necessarily the first to do said things and
there have of course been various improvements on the cited systems. We have
decided to select the ones that we felt were the most famous and representative
ones (especially at the time or their release or announcement). It is also
notable how little time passes from the release of a research paper
(Attention is All You Need) to the commercial implementation of it's ideas
(ChatGPT).

Another interesting fact is that wile most of the cited systems are closed
source the open source community is (with a bit of lag) keeping up with
research and commercial systems. This we feel is very important for the
democratization of this technologies. Almost each of the cited technologies has
an open source counter, Autopilot has OpenPilot, GPT-3 has LLAMA, ChatGPT has
OpenAssistant and DALL-E has StableDiffusion.

\section{A Formal Introduction}

\TODO{strivere questa cosa in funzione del dataset $\mathcal D$ che potrebbe o
non potrebbe contenere le etichette, inoltre vorrei usare una notazione che Ã¨ un
misto di ML basic, Deep Learning e understanding machine learning}
In general in machine learning (and deep learning)  we have a set of data points
$\mathcal X$, a set of labels for those points $\mathcal Y$\footnote{This is
not always necessary, there exits a set of algorithms (so called \emph
{unsupervised}) that can learn useful properties from unlabeled data. But, as
we are going to see, you still need a loss function that for the unsupervised
case is just a function of the data.}, an hypothesis space $\mathcal H$ that
contains functions $h : \mathcal X \to \mathcal Y$ and a loss function $\ell
(x, y, h)$ that measures how much the predicted label is off from the real
one.

We are going to assume that each pair $(x, y)$ is the realization of a random
variable coming from the joint probability distribution $p(x, y)$ (which is the
same for all pair of points even if the notation does not make this clear).

We are going to assume that each $x \in \mathcal X$ is the \emph{realization} of
an random variable and that each of them is i.i.d. This assumption allows us to
define the quality of an hypothesis as the \emph{risk} of the expectation of the
loss as a function two random variables $x$ and $y$ (of course the lower risk
the better quality the hypothesis is).

\begin{displaymath}
	\argmin_{h \in \mathcal H} \mathbf E [\ell(h(x), y)]
\end{displaymath}

The problem is that the true distribution $p(x, y)$ is unknown...

\begin{displaymath}
	\hat h \in \argmin_{h \in \mathcal H} \hat \ell(h|\mathcal D)
	= \argmin_{h \in \mathcal H} \frac 1 m \sum_{i=1}^m \ell(h(x^{(i)}), y^{(i)})
\end{displaymath}

% https://en.wikipedia.org/wiki/Frobenius_inner_product
The Frobenius inner product (which like any inner product is a sesquilinear
form) and the norm induced by it:

\begin{equation}
	\langle A,B \rangle_F = \sqrt{\trace(A^TB)} = \sqrt{\sum_{i,j} a_{ij}b_{ij}}
\end{equation}

\begin{equation}
	\langle A,A \rangle_F = ||A||_F = \sqrt{\trace(A^TA)} = \sqrt{\sum_{i,j} a_{ij}^2}
\end{equation}

% https://en.wikipedia.org/wiki/Hermitian_adjoint
% https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space ???

We start with a brief introduction to machine learning, then neural networks and
finally talk about deep learning.

After having (briefly) established that the previous three things have started
to work really well in the previous years. We can conclude this chapter by
asking the question of how do we learn \emph{fast} with computers? Which kinds
of algorithms/software and hardware do we need?

\subsection{Evaluating Derivatives}

\chapter{Hardware acceleration and GPU architecture}

Linear algebra operations, which are at the core of deep learning, are
\emph{compute intensive}. There are many recent hardware
advancements/experiments in the recent period (TPU). The most used accelerator
is the GPU

\part{Implementation}

\end{document}

\chapter{Mathematical Optimization and Automatic Differentiation}

To optimize deep learning models we use stochastic gradient descent. The most
important thing in this procedure is to calculate the gradient (of the loss).
This is done using Automatic Differentiation, which is neither numerical nor
symbolical differentiation.

% Thanks Ian for clarifiing this in the Deep Learning book.
The trace function is going to be a useful instrument in our calculations since
it allows us to avoid the summation notation and resort to express many
operations as a combination of matrix multiplication and the trace operation
itself. This fact, together with the many useful invariants/properties of the
trace function allows us to perform our calculations in ``pure'' matrix
calculus. So this rather ingenuous looking function is going to be our work
horse.

\iffalse
From Wikipedia (which has a good introduction to the topic):
https://en.wikipedia.org/wiki/Automatic_differentiation
Automatic differentiation exploits the fact that every computer calculation, no
matter how complicated, executes a sequence of elementary arithmetic
operations (addition, subtraction, multiplication, division, etc.) and
elementary functions (exp, log, sin, cos, etc.). By applying the chain rule
repeatedly to these operations, partial derivatives of arbitrary order can be
computed automatically, accurately to working precision, and using at most a
small constant factor of more arithmetic operations than the original program.

Difference from other differentiation methods[edit source]

Automatic differentiation is distinct from symbolic differentiation and
numerical differentiation. Symbolic differentiation faces the difficulty of
converting a computer program into a single mathematical expression and can
lead to inefficient code. Numerical differentiation (the method of finite
differences) can introduce round-off errors in the discretization process and
cancellation. Both of these classical methods have problems with calculating
higher derivatives, where complexity and errors increase. Finally, both of
these classical methods are slow at computing partial derivatives of a function
with respect to many inputs, as is needed for gradient-based optimization
algorithms. Automatic differentiation solves all of these problems.
\fi

\dots

Matrix derivatives are a mess. There is no other other way to put it. To make
things as clear as possible we are going to use Householder notation for the
letters representing scalar, vectors and matrices where are going to assume
vectors to be columns by default and that an $m \times n$ matrix has $m$ rows
and $n$ columns. When we take the derivative of a function we mean the
$\alpha$-derivative~\cite{magnus}, \TODO{clarify the layout convention used for
derivatives}.

Since this is a computer science thesis, to avoid any possible confusion, the
layout in memory of the matrices is going to be row-major (therefore C's
layout convention not Fortran's one).

Differentiation can be tackled in various ways on computers\dots The ``naive''
approach it to use the definition and do it in a numerical way. This approach
has numerical issues and an algorithm that works for a given function may not
work for another one. It would be cool to an algorithm that works by
construction i.e. if we can write a function in our formalism then we can take
its derivative with respect to any argument.

\chapter{Currently used libraries and frameworks}

The usual suspects like PyTorch and TensorFlow. But also some less known stuff
like ADOL-C.
